{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97203200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[98603]: Class CaptureDelegate is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1647f0860) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x338712480). One of the two will be used. Which one is undefined.\n",
      "objc[98603]: Class CVWindow is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x15924ca68) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3387124d0). One of the two will be used. Which one is undefined.\n",
      "objc[98603]: Class CVView is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x15924ca90) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x3387124f8). One of the two will be used. Which one is undefined.\n",
      "objc[98603]: Class CVSlider is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x15924cab8) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x338712520). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e927fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # get holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # get drawing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f32a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert image color to RGB\n",
    "    image.flags.writeable = False                  # don't show this image in feed\n",
    "    results = model.process(image)                 # make holistics prediction\n",
    "    image.flags.writeable = True                   # make image writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # convert back to BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d104c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abseiling', 'acting in play', 'adjusting glasses', 'air drumming',\n",
       "       'alligator wrestling', 'answering questions', 'applauding',\n",
       "       'applying cream', 'archaeological excavation', 'archery',\n",
       "       'arguing', 'arm wrestling', 'arranging flowers',\n",
       "       'assembling bicycle', 'assembling computer',\n",
       "       'attending conference', 'auctioning', 'backflip (human)',\n",
       "       'baking cookies', 'bandaging'], dtype='<U49')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path = tf.keras.utils.get_file(\n",
    "    fname='labels.txt',\n",
    "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
    ")\n",
    "labels_path = pathlib.Path(labels_path)\n",
    "\n",
    "lines = labels_path.read_text().splitlines()\n",
    "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
    "KINETICS_600_LABELS[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b998c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'experiment_subset'\n",
    "# subset_paths = {'train': pathlib.Path(os.path.join(DATA_PATH, 'train')),\n",
    "#                 'val': pathlib.Path(os.path.join(DATA_PATH, 'val')),\n",
    "#                 'test': pathlib.Path(os.path.join(DATA_PATH, 'test'))}\n",
    "# print(subset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f04ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frames_from_video_file(video_path, n_frames, output_size = (256,256)):\n",
    "#   \"\"\" Creates frames from each video file present for each category.\n",
    "\n",
    "#     Args:\n",
    "#       video_path: File path to the video.\n",
    "#       n_frames: Number of frames to be created per video file.\n",
    "#       output_size: Pixel size of the output frame image.\n",
    "\n",
    "#     Return:\n",
    "#       An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#   \"\"\"\n",
    "#   # Read each frame by frame\n",
    "#   result = []\n",
    "#   src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "#   video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#   # If the number of frames wanted is greater than the length of the video, then start from beginning\n",
    "#   if n_frames > video_length:\n",
    "#     start = 0\n",
    "#   else:\n",
    "#     # Otherwise, start at another random point within the video\n",
    "#     max_start = video_length - n_frames\n",
    "#     start = random.randint(0, max_start)\n",
    "\n",
    "#   src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "#   for _ in range(n_frames):\n",
    "#     ret, frame = src.read()\n",
    "#     if ret:\n",
    "#       frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "#       frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "#       result.append(frame)\n",
    "#     else:\n",
    "#       result.append(np.zeros_like(result[0]))\n",
    "#   src.release()\n",
    "#   # Ensure that the color scheme is not inverted\n",
    "#   result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "#   return result\n",
    "\n",
    "# class FrameGenerator:\n",
    "#   def __init__(self, path, n_frames):\n",
    "#     \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "#       Args:\n",
    "#         path: Video file paths.\n",
    "#         classes: List of labels for classification.\n",
    "#     \"\"\"\n",
    "#     self.path = path\n",
    "#     self.n_frames = n_frames\n",
    "#     self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "#     self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "#   def get_files_and_class_names(self):\n",
    "#     video_paths = list(self.path.glob('*/*.mp4'))\n",
    "#     classes = [p.parent.name for p in video_paths] \n",
    "#     return video_paths, classes\n",
    "\n",
    "#   def __call__(self):\n",
    "#     video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "#     pairs = list(zip(video_paths, classes))\n",
    "\n",
    "#     random.shuffle(pairs)\n",
    "\n",
    "#     for path, name in pairs:\n",
    "#       video_frames = frames_from_video_file(path, self.n_frames) \n",
    "#       label = self.class_ids_for_name[name] # Encode labels\n",
    "#       yield video_frames, label\n",
    "    \n",
    "# def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
    "#   \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
    "\n",
    "#   Args:\n",
    "#     probs: probability tensor of shape (num_frames, num_classes) that represents\n",
    "#       the probability of each class on each frame.\n",
    "#     k: the number of top predictions to select.\n",
    "#     label_map: a list of labels to map logit indices to label strings.\n",
    "\n",
    "#   Returns:\n",
    "#     a tuple of the top-k labels and probabilities.\n",
    "#   \"\"\"\n",
    "#   # Sort predictions to find top_k\n",
    "#   top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
    "#   # collect the labels of top_k predictions\n",
    "#   top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
    "#   # decode lablels\n",
    "#   top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
    "#   # top_k probabilities of the predictions\n",
    "#   top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
    "#   return tuple(zip(top_labels, top_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "#                     tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "# train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 12),\n",
    "#                                           output_signature = output_signature)\n",
    "# train_frames, train_labels = next(iter(train_ds))\n",
    "# print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "# print(f'Shape of training labels: {train_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a286473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61830c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# id = 'a0'\n",
    "# mode = 'stream'\n",
    "# version = '3'\n",
    "# hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
    "# model = hub.load(hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ebeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = model.signatures['init_states'].pretty_printed_signature().splitlines()\n",
    "# lines = lines[:10]\n",
    "# lines.append('      ...')\n",
    "# print('.\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241cb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_states = model.init_states(train_frames[tf.newaxis].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Insert your video clip here\n",
    "# video = train_frames\n",
    "# images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
    "\n",
    "# all_logits = []\n",
    "\n",
    "# # To run on a video, pass in one frame at a time\n",
    "# states = init_states\n",
    "# for image in tqdm.tqdm(images):\n",
    "#   # predictions for each frame\n",
    "#   logits, states = model({**states, 'image': image})\n",
    "#   all_logits.append(logits)\n",
    "\n",
    "# # concatinating all the logits\n",
    "# logits = tf.concat(all_logits, 0)\n",
    "# # estimating probabilities\n",
    "# probs = tf.nn.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_probs = probs[-1]\n",
    "# print('Top_k predictions and their probablities\\n')\n",
    "# for label, p in get_top_k(final_probs):\n",
    "#   print(f'{label:20s}: {p:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b2e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74905514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d756d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d61f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20593bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eacf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9287a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5f2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "#from official.projects.movinet.modeling import movinet\n",
    "#from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86012f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'a0'\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# # Create backbone and model.\n",
    "# use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
    "# backbone = movinet.Movinet(\n",
    "#     model_id=model_id,\n",
    "#     causal=True,\n",
    "#     conv_type='2plus1d',\n",
    "#     se_type='2plus3d',\n",
    "#     activation='hard_swish',\n",
    "#     gating_activation='hard_sigmoid',\n",
    "#     use_positional_encoding=use_positional_encoding,\n",
    "#     use_external_states=True,\n",
    "# )\n",
    "\n",
    "# # Create a movinet classifier using this backbone.\n",
    "# model = movinet_model.MovinetClassifier(\n",
    "#     backbone,\n",
    "#     num_classes=3,\n",
    "#     output_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8ab84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# model_id = 'a0'\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# movinet_hub_url = f'https://tfhub.dev/tensorflow/movinet/{model_id}/stream/kinetics-600/classification/3'\n",
    "\n",
    "# movinet_hub_model = hub.KerasLayer(movinet_hub_url, trainable=True)\n",
    "\n",
    "# pretrained_weights = {w.name: w for w in movinet_hub_model.weights}\n",
    "\n",
    "# model_weights = {w.name: w for w in model.weights}\n",
    "\n",
    "# for name in pretrained_weights:\n",
    "#     model_weights[name].assign(pretrained_weights[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92486e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 18:18:54.208372: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-10-17 18:18:54.300634: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x301caa9b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-17 18:18:54.300651: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-17 18:18:54.359344: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Wrap the Movinet model in a Keras model so that it can be finetuned.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstates_input, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image_input}\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# # Input layer for the frame sequence\n",
    "# image_input = tf.keras.layers.Input(\n",
    "#     shape=[None, None, None, 3],\n",
    "#     dtype=tf.float32,\n",
    "#     name='image')\n",
    "\n",
    "# # Input layers for the different model states.\n",
    "# init_states_fn = movinet_hub_model.resolved_object.signatures['init_states']\n",
    "\n",
    "\n",
    "# state_shapes = {\n",
    "#     name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "#     for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "# }\n",
    "\n",
    "# states_input = {\n",
    "#     name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "#     for name, (shape, dtype) in state_shapes.items()\n",
    "# }\n",
    "\n",
    "# # Wrap the Movinet model in a Keras model so that it can be finetuned.\n",
    "\n",
    "# inputs = {**states_input, 'image': image_input}\n",
    "\n",
    "# outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Model):\n",
    "#     def train_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         if len(data) == 3:\n",
    "#             x, y, sample_weight = data\n",
    "#         else:\n",
    "#             sample_weight = None\n",
    "#             x, y = data\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred, states = self(x, training=True)  # Forward pass\n",
    "            \n",
    "#             # Compute the loss value\n",
    "#             # (the loss function is configured in `compile()`)\n",
    "#             loss = self.compiled_loss(y, pred, regularization_losses=self.losses, sample_weight=sample_weight)\n",
    "\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(y, pred, sample_weight=sample_weight)\n",
    "        \n",
    "#         # Return a dict mapping metric names to current value\n",
    "#         return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd34aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomModel(inputs, outputs, name='movinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57080e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "# model.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_states = init_states_fn(tf.shape(tf.ones([BATCH_SIZE, NBFRAME, SIZE[0], SIZE[1], CHANNELS])))\n",
    "# finetune data in the format ({**init_states, 'image': frame_sequence}, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43335e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_url = \"https://tfhub.dev/tensorflow/movinet/a0/stream/kinetics-600/classification/3\"\n",
    "\n",
    "encoder = hub.KerasLayer(hub_url, trainable=False)\n",
    "\n",
    "# Define the image (video) input\n",
    "image_input = tf.keras.layers.Input(\n",
    "    shape=[None, None, None, 3],\n",
    "    dtype=tf.float32,\n",
    "    name='image')\n",
    "\n",
    "# Define the state inputs, which is a dict that maps state names to tensors.\n",
    "init_states_fn = encoder.resolved_object.signatures['init_states']\n",
    "state_shapes = {\n",
    "    name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "    for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "}\n",
    "states_input = {\n",
    "    name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "    for name, (shape, dtype) in state_shapes.items()\n",
    "}\n",
    "\n",
    "# The inputs to the model are the states and the video\n",
    "inputs = {**states_input, 'image': image_input}\n",
    "\n",
    "outputs = encoder(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='movinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c58e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([6], shape=(1,), dtype=int64)\n",
      "['applauding']\n"
     ]
    }
   ],
   "source": [
    "# Create your example input here.\n",
    "# Refer to the description or paper for recommended input shapes.\n",
    "example_input = tf.random.normal([1, 8, 172, 172, 3])\n",
    "\n",
    "# Split the video into individual frames.\n",
    "# Note: we can also split into larger clips as well (e.g., 8-frame clips).\n",
    "# Running on larger clips will slightly reduce latency overhead, but\n",
    "# will consume more memory.\n",
    "frames = tf.split(example_input, example_input.shape[1], axis=1)\n",
    "\n",
    "# Initialize the dict of states. All state tensors are initially zeros.\n",
    "init_states = init_states_fn(tf.shape(example_input))\n",
    "\n",
    "# Run the model prediction by looping over each frame.\n",
    "states = init_states\n",
    "predictions = []\n",
    "for frame in frames:\n",
    "    output, states = model({**states, 'image': frame})\n",
    "    predictions.append(output)\n",
    "\n",
    "# The video classification will simply be the last output of the model.\n",
    "final_prediction = tf.argmax(predictions[-1], -1)\n",
    "\n",
    "# Alternatively, we can run the network on the entire input video.\n",
    "# The output should be effectively the same\n",
    "# (but it may differ a small amount due to floating point errors).\n",
    "#non_streaming_output, _ = model({**init_states, 'image': example_input})\n",
    "#non_streaming_prediction = tf.argmax(non_streaming_output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01501e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beatboxing\n",
      "beatboxing playing didgeridoo\n",
      "beatboxing playing didgeridoo playing harmonica\n",
      "beatboxing playing didgeridoo playing harmonica playing trumpet\n",
      "beatboxing playing didgeridoo playing harmonica playing trumpet playing trumpet\n",
      "playing didgeridoo playing harmonica playing trumpet playing trumpet playing didgeridoo\n",
      "playing harmonica playing trumpet playing trumpet playing didgeridoo playing didgeridoo\n",
      "playing trumpet playing trumpet playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing trumpet playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo staring\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo staring staring\n",
      "playing didgeridoo playing didgeridoo staring staring staring\n",
      "playing didgeridoo staring staring staring staring\n",
      "staring staring staring staring playing didgeridoo\n",
      "staring staring staring playing didgeridoo staring\n",
      "staring staring playing didgeridoo staring playing didgeridoo\n",
      "staring playing didgeridoo staring playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo staring playing didgeridoo playing didgeridoo whistling\n",
      "staring playing didgeridoo playing didgeridoo whistling whistling\n",
      "playing didgeridoo playing didgeridoo whistling whistling whistling\n",
      "playing didgeridoo whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling playing didgeridoo\n",
      "whistling whistling whistling playing didgeridoo playing didgeridoo\n",
      "whistling whistling playing didgeridoo playing didgeridoo whistling\n",
      "whistling playing didgeridoo playing didgeridoo whistling playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo whistling playing didgeridoo playing pan pipes\n",
      "playing didgeridoo whistling playing didgeridoo playing pan pipes playing didgeridoo\n",
      "whistling playing didgeridoo playing pan pipes playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing pan pipes playing didgeridoo playing didgeridoo playing flute\n",
      "playing pan pipes playing didgeridoo playing didgeridoo playing flute playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing flute playing didgeridoo playing flute\n",
      "playing didgeridoo playing flute playing didgeridoo playing flute playing flute\n",
      "playing flute playing didgeridoo playing flute playing flute playing didgeridoo\n",
      "playing didgeridoo playing flute playing flute playing didgeridoo playing didgeridoo\n",
      "playing flute playing flute playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing flute playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo staring\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo staring playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo staring playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo staring playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "staring playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo staring\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo staring staring\n",
      "playing didgeridoo playing didgeridoo staring staring staring\n",
      "playing didgeridoo staring staring staring playing didgeridoo\n",
      "staring staring staring playing didgeridoo playing didgeridoo\n",
      "staring staring playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "staring playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing didgeridoo playing ocarina\n",
      "playing didgeridoo playing didgeridoo playing didgeridoo playing ocarina playing ocarina\n",
      "playing didgeridoo playing didgeridoo playing ocarina playing ocarina playing ocarina\n",
      "playing didgeridoo playing ocarina playing ocarina playing ocarina raising eyebrows\n",
      "playing ocarina playing ocarina playing ocarina raising eyebrows raising eyebrows\n",
      "playing ocarina playing ocarina raising eyebrows raising eyebrows whistling\n",
      "playing ocarina raising eyebrows raising eyebrows whistling whistling\n",
      "raising eyebrows raising eyebrows whistling whistling whistling\n",
      "raising eyebrows whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling whistling\n",
      "whistling whistling whistling whistling playing pan pipes\n",
      "whistling whistling whistling playing pan pipes playing pan pipes\n",
      "whistling whistling playing pan pipes playing pan pipes playing pan pipes\n",
      "whistling playing pan pipes playing pan pipes playing pan pipes playing pan pipes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing pan pipes playing pan pipes playing pan pipes playing pan pipes cosplaying\n",
      "playing pan pipes playing pan pipes playing pan pipes cosplaying cosplaying\n",
      "playing pan pipes playing pan pipes cosplaying cosplaying talking on cell phone\n",
      "playing pan pipes cosplaying cosplaying talking on cell phone playing harmonica\n",
      "cosplaying cosplaying talking on cell phone playing harmonica playing harmonica\n",
      "cosplaying talking on cell phone playing harmonica playing harmonica playing harmonica\n",
      "talking on cell phone playing harmonica playing harmonica playing harmonica smoking pipe\n",
      "playing harmonica playing harmonica playing harmonica smoking pipe smoking hookah\n",
      "playing harmonica playing harmonica smoking pipe smoking hookah smoking hookah\n",
      "playing harmonica smoking pipe smoking hookah smoking hookah staring\n",
      "smoking pipe smoking hookah smoking hookah staring staring\n",
      "smoking hookah smoking hookah staring staring staring\n",
      "smoking hookah staring staring staring staring\n",
      "staring staring staring staring staring\n",
      "staring staring staring staring staring\n",
      "staring staring staring staring staring\n",
      "staring staring staring staring staring\n",
      "staring staring staring staring staring\n",
      "staring staring staring staring staring\n"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "out = np.array([0])\n",
    "threshold = 0\n",
    "num_frames = 8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# access mp model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # get holistic detections\n",
    "        image, results = mp_detection(frame, holistic)\n",
    "        \n",
    "        # draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        frame = cv2.resize(frame, (172, 172))/255\n",
    "\n",
    "        #keypoints = extract_keypoints(results)\n",
    "        sequence.append(frame)\n",
    "        #sequence = tf.concat([sequence, frame], 0)\n",
    "        sequence = sequence[-num_frames:]\n",
    "        #print(tf.convert_to_tensor(sequence).shape)\n",
    "\n",
    "        if len(sequence) == num_frames:\n",
    "            #out = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            #print(actions[np.argmax(out)])\n",
    "            batch = tf.convert_to_tensor(sequence, dtype = 'float32')[tf.newaxis]\n",
    "            #out = infer(batch[tf.newaxis])['dense_5'].numpy()[0]\n",
    "            frames = tf.split(batch, batch.shape[1], axis=1)\n",
    "            init_states = init_states_fn(tf.shape(batch))\n",
    "            states = init_states\n",
    "            predictions = []\n",
    "            for f in frames:\n",
    "                output, states = model({**states, 'image': f})\n",
    "                predictions.append(output)\n",
    "                \n",
    "            out = list(KINETICS_600_LABELS[tf.argmax(predictions[-1], -1)])\n",
    "            \n",
    "            sentence += out\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "                \n",
    "            \n",
    "\n",
    "#         if out[np.argmax(out)] > threshold:\n",
    "#             if len(sentence) > 0:\n",
    "#                 if actions[np.argmax(out)] != sentence[-1]:\n",
    "#                     sentence.append(actions[np.argmax(out)])\n",
    "#             else:\n",
    "#                 sentence.append(actions[np.argmax(out)])\n",
    "\n",
    "#         if len(sentence) > 5:\n",
    "#             sentence = sentence[-5:]\n",
    "        \n",
    "            print(' '.join(sentence))\n",
    "        \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # press q to break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef83a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': PosixPath('experiment_subset/train'), 'val': PosixPath('experiment_subset/val'), 'test': PosixPath('experiment_subset/test')}\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'experiment_subset'\n",
    "subset_paths = {'train': pathlib.Path(os.path.join(DATA_PATH, 'train')),\n",
    "                'val': pathlib.Path(os.path.join(DATA_PATH, 'val')),\n",
    "                'test': pathlib.Path(os.path.join(DATA_PATH, 'test'))}\n",
    "print(subset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d667c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(set(p.name for p in pathlib.Path(os.path.join(DATA_PATH, 'train')).iterdir() if p.is_dir()))\n",
    "label_map = dict((name, idx) for idx, name in enumerate(class_names))\n",
    "actions = list(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae3cb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'love': 1, 'thank you': 2}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
