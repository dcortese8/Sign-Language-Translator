{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97203200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[30036]: Class CaptureDelegate is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1200c8860) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x169902480). One of the two will be used. Which one is undefined.\n",
      "objc[30036]: Class CVWindow is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b10a68) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x1699024d0). One of the two will be used. Which one is undefined.\n",
      "objc[30036]: Class CVView is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b10a90) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x1699024f8). One of the two will be used. Which one is undefined.\n",
      "objc[30036]: Class CVSlider is implemented in both /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b10ab8) and /Users/dev/Sign-Language-Translator/env/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x169902520). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e927fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # get holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # get drawing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f32a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert image color to RGB\n",
    "    image.flags.writeable = False                  # don't show this image in feed\n",
    "    results = model.process(image)                 # make holistics prediction\n",
    "    image.flags.writeable = True                   # make image writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # convert back to BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d104c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abseiling', 'acting in play', 'adjusting glasses', 'air drumming',\n",
       "       'alligator wrestling', 'answering questions', 'applauding',\n",
       "       'applying cream', 'archaeological excavation', 'archery',\n",
       "       'arguing', 'arm wrestling', 'arranging flowers',\n",
       "       'assembling bicycle', 'assembling computer',\n",
       "       'attending conference', 'auctioning', 'backflip (human)',\n",
       "       'baking cookies', 'bandaging'], dtype='<U49')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path = tf.keras.utils.get_file(\n",
    "    fname='labels.txt',\n",
    "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
    ")\n",
    "labels_path = pathlib.Path(labels_path)\n",
    "\n",
    "lines = labels_path.read_text().splitlines()\n",
    "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
    "KINETICS_600_LABELS[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b998c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'experiment_subset'\n",
    "# subset_paths = {'train': pathlib.Path(os.path.join(DATA_PATH, 'train')),\n",
    "#                 'val': pathlib.Path(os.path.join(DATA_PATH, 'val')),\n",
    "#                 'test': pathlib.Path(os.path.join(DATA_PATH, 'test'))}\n",
    "# print(subset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f04ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frames_from_video_file(video_path, n_frames, output_size = (256,256)):\n",
    "#   \"\"\" Creates frames from each video file present for each category.\n",
    "\n",
    "#     Args:\n",
    "#       video_path: File path to the video.\n",
    "#       n_frames: Number of frames to be created per video file.\n",
    "#       output_size: Pixel size of the output frame image.\n",
    "\n",
    "#     Return:\n",
    "#       An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#   \"\"\"\n",
    "#   # Read each frame by frame\n",
    "#   result = []\n",
    "#   src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "#   video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#   # If the number of frames wanted is greater than the length of the video, then start from beginning\n",
    "#   if n_frames > video_length:\n",
    "#     start = 0\n",
    "#   else:\n",
    "#     # Otherwise, start at another random point within the video\n",
    "#     max_start = video_length - n_frames\n",
    "#     start = random.randint(0, max_start)\n",
    "\n",
    "#   src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "#   for _ in range(n_frames):\n",
    "#     ret, frame = src.read()\n",
    "#     if ret:\n",
    "#       frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "#       frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "#       result.append(frame)\n",
    "#     else:\n",
    "#       result.append(np.zeros_like(result[0]))\n",
    "#   src.release()\n",
    "#   # Ensure that the color scheme is not inverted\n",
    "#   result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "#   return result\n",
    "\n",
    "# class FrameGenerator:\n",
    "#   def __init__(self, path, n_frames):\n",
    "#     \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "#       Args:\n",
    "#         path: Video file paths.\n",
    "#         classes: List of labels for classification.\n",
    "#     \"\"\"\n",
    "#     self.path = path\n",
    "#     self.n_frames = n_frames\n",
    "#     self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "#     self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "#   def get_files_and_class_names(self):\n",
    "#     video_paths = list(self.path.glob('*/*.mp4'))\n",
    "#     classes = [p.parent.name for p in video_paths] \n",
    "#     return video_paths, classes\n",
    "\n",
    "#   def __call__(self):\n",
    "#     video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "#     pairs = list(zip(video_paths, classes))\n",
    "\n",
    "#     random.shuffle(pairs)\n",
    "\n",
    "#     for path, name in pairs:\n",
    "#       video_frames = frames_from_video_file(path, self.n_frames) \n",
    "#       label = self.class_ids_for_name[name] # Encode labels\n",
    "#       yield video_frames, label\n",
    "    \n",
    "# def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
    "#   \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
    "\n",
    "#   Args:\n",
    "#     probs: probability tensor of shape (num_frames, num_classes) that represents\n",
    "#       the probability of each class on each frame.\n",
    "#     k: the number of top predictions to select.\n",
    "#     label_map: a list of labels to map logit indices to label strings.\n",
    "\n",
    "#   Returns:\n",
    "#     a tuple of the top-k labels and probabilities.\n",
    "#   \"\"\"\n",
    "#   # Sort predictions to find top_k\n",
    "#   top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
    "#   # collect the labels of top_k predictions\n",
    "#   top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
    "#   # decode lablels\n",
    "#   top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
    "#   # top_k probabilities of the predictions\n",
    "#   top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
    "#   return tuple(zip(top_labels, top_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edd391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "#                     tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "# train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 12),\n",
    "#                                           output_signature = output_signature)\n",
    "# train_frames, train_labels = next(iter(train_ds))\n",
    "# print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "# print(f'Shape of training labels: {train_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a286473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61830c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de35b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# id = 'a0'\n",
    "# mode = 'stream'\n",
    "# version = '3'\n",
    "# hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
    "# model = hub.load(hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "954ebeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = model.signatures['init_states'].pretty_printed_signature().splitlines()\n",
    "# lines = lines[:10]\n",
    "# lines.append('      ...')\n",
    "# print('.\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241cb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_states = model.init_states(train_frames[tf.newaxis].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441e85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Insert your video clip here\n",
    "# video = train_frames\n",
    "# images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
    "\n",
    "# all_logits = []\n",
    "\n",
    "# # To run on a video, pass in one frame at a time\n",
    "# states = init_states\n",
    "# for image in tqdm.tqdm(images):\n",
    "#   # predictions for each frame\n",
    "#   logits, states = model({**states, 'image': image})\n",
    "#   all_logits.append(logits)\n",
    "\n",
    "# # concatinating all the logits\n",
    "# logits = tf.concat(all_logits, 0)\n",
    "# # estimating probabilities\n",
    "# probs = tf.nn.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d312a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_probs = probs[-1]\n",
    "# print('Top_k predictions and their probablities\\n')\n",
    "# for label, p in get_top_k(final_probs):\n",
    "#   print(f'{label:20s}: {p:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b2e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74905514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d756d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d61f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20593bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eacf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9287a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a5f2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "#from official.projects.movinet.modeling import movinet\n",
    "#from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86012f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'a0'\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# # Create backbone and model.\n",
    "# use_positional_encoding = model_id in {'a3', 'a4', 'a5'}\n",
    "# backbone = movinet.Movinet(\n",
    "#     model_id=model_id,\n",
    "#     causal=True,\n",
    "#     conv_type='2plus1d',\n",
    "#     se_type='2plus3d',\n",
    "#     activation='hard_swish',\n",
    "#     gating_activation='hard_sigmoid',\n",
    "#     use_positional_encoding=use_positional_encoding,\n",
    "#     use_external_states=True,\n",
    "# )\n",
    "\n",
    "# # Create a movinet classifier using this backbone.\n",
    "# model = movinet_model.MovinetClassifier(\n",
    "#     backbone,\n",
    "#     num_classes=3,\n",
    "#     output_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe8ab84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_id = 'a0'\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# movinet_hub_url = f'https://tfhub.dev/tensorflow/movinet/{model_id}/stream/kinetics-600/classification/3'\n",
    "\n",
    "# movinet_hub_model = hub.KerasLayer(movinet_hub_url, trainable=True)\n",
    "\n",
    "# pretrained_weights = {w.name: w for w in movinet_hub_model.weights}\n",
    "\n",
    "# model_weights = {w.name: w for w in model.weights}\n",
    "\n",
    "# for name in pretrained_weights:\n",
    "#     model_weights[name].assign(pretrained_weights[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92486e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input layer for the frame sequence\n",
    "# image_input = tf.keras.layers.Input(\n",
    "#     shape=[None, None, None, 3],\n",
    "#     dtype=tf.float32,\n",
    "#     name='image')\n",
    "\n",
    "# # Input layers for the different model states.\n",
    "# init_states_fn = movinet_hub_model.resolved_object.signatures['init_states']\n",
    "\n",
    "\n",
    "# state_shapes = {\n",
    "#     name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "#     for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "# }\n",
    "\n",
    "# states_input = {\n",
    "#     name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "#     for name, (shape, dtype) in state_shapes.items()\n",
    "# }\n",
    "\n",
    "# # Wrap the Movinet model in a Keras model so that it can be finetuned.\n",
    "\n",
    "# inputs = {**states_input, 'image': image_input}\n",
    "\n",
    "# outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de71c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Model):\n",
    "#     def train_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         if len(data) == 3:\n",
    "#             x, y, sample_weight = data\n",
    "#         else:\n",
    "#             sample_weight = None\n",
    "#             x, y = data\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred, states = self(x, training=True)  # Forward pass\n",
    "            \n",
    "#             # Compute the loss value\n",
    "#             # (the loss function is configured in `compile()`)\n",
    "#             loss = self.compiled_loss(y, pred, regularization_losses=self.losses, sample_weight=sample_weight)\n",
    "\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(y, pred, sample_weight=sample_weight)\n",
    "        \n",
    "#         # Return a dict mapping metric names to current value\n",
    "#         return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd34aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomModel(inputs, outputs, name='movinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57080e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "# model.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111b533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_states = init_states_fn(tf.shape(tf.ones([BATCH_SIZE, NBFRAME, SIZE[0], SIZE[1], CHANNELS])))\n",
    "# finetune data in the format ({**init_states, 'image': frame_sequence}, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f43335e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 14:30:32.629842: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-10-21 14:30:32.703299: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x120effe30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-21 14:30:32.703316: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-21 14:30:32.765627: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "hub_url = \"https://tfhub.dev/tensorflow/movinet/a0/stream/kinetics-600/classification/3\"\n",
    "\n",
    "encoder = hub.KerasLayer(hub_url, trainable=False)\n",
    "\n",
    "# Define the image (video) input\n",
    "image_input = tf.keras.layers.Input(\n",
    "    shape=[None, None, None, 3],\n",
    "    dtype=tf.float32,\n",
    "    name='image')\n",
    "\n",
    "# Define the state inputs, which is a dict that maps state names to tensors.\n",
    "init_states_fn = encoder.resolved_object.signatures['init_states']\n",
    "state_shapes = {\n",
    "    name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "    for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "}\n",
    "states_input = {\n",
    "    name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "    for name, (shape, dtype) in state_shapes.items()\n",
    "}\n",
    "\n",
    "# The inputs to the model are the states and the video\n",
    "inputs = {**states_input, 'image': image_input}\n",
    "\n",
    "outputs = encoder(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='movinet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81c58e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your example input here.\n",
    "# Refer to the description or paper for recommended input shapes.\n",
    "example_input = tf.random.normal([1, 8, 172, 172, 3])\n",
    "\n",
    "# Split the video into individual frames.\n",
    "# Note: we can also split into larger clips as well (e.g., 8-frame clips).\n",
    "# Running on larger clips will slightly reduce latency overhead, but\n",
    "# will consume more memory.\n",
    "frames = tf.split(example_input, example_input.shape[1], axis=1)\n",
    "\n",
    "# Initialize the dict of states. All state tensors are initially zeros.\n",
    "init_states = init_states_fn(tf.shape(example_input))\n",
    "\n",
    "# Run the model prediction by looping over each frame.\n",
    "states = init_states\n",
    "predictions = []\n",
    "for frame in frames:\n",
    "    output, states = model({**states, 'image': frame})\n",
    "    predictions.append(output)\n",
    "\n",
    "# The video classification will simply be the last output of the model.\n",
    "final_prediction = tf.argmax(predictions[-1], -1)\n",
    "\n",
    "# Alternatively, we can run the network on the entire input video.\n",
    "# The output should be effectively the same\n",
    "# (but it may differ a small amount due to floating point errors).\n",
    "#non_streaming_output, _ = model({**init_states, 'image': example_input})\n",
    "#non_streaming_prediction = tf.argmax(non_streaming_output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90014ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_viz(res, actions, input_frame):\n",
    "    colors = [[217, 95, 156], [95, 156, 217], [156, 217, 95]]\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, i, prob in res:\n",
    "        #print(prob)\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, ' '.join([actions[i], str(np.round(prob*100, 2)), '%']), (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5a3e14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_k_inds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m), \u001b[43mtop_k_inds\u001b[49m[\u001b[38;5;241m0\u001b[39m], top_k_vals[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m plot_viz(z, KINETICS_600_LABELS, image)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_k_inds' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "z = zip(range(0, 3), top_k_inds[0], top_k_vals[0])\n",
    "image = plot_viz(z, KINETICS_600_LABELS, image)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01501e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "out = np.array([0])\n",
    "threshold = 0\n",
    "num_frames = 8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# access mp model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # get holistic detections\n",
    "        image, results = mp_detection(frame, holistic)\n",
    "        \n",
    "        # draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        frame = cv2.resize(frame, (172, 172))/255\n",
    "\n",
    "        #keypoints = extract_keypoints(results)\n",
    "        sequence.append(frame)\n",
    "        #sequence = tf.concat([sequence, frame], 0)\n",
    "        sequence = sequence[-num_frames:]\n",
    "        #print(tf.convert_to_tensor(sequence).shape)\n",
    "\n",
    "        if len(sequence) == num_frames:\n",
    "            #out = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            #print(actions[np.argmax(out)])\n",
    "            batch = tf.convert_to_tensor(sequence, dtype = 'float32')[tf.newaxis]\n",
    "            #out = infer(batch[tf.newaxis])['dense_5'].numpy()[0]\n",
    "            frames = tf.split(batch, batch.shape[1], axis=1)\n",
    "            init_states = init_states_fn(tf.shape(batch))\n",
    "            states = init_states\n",
    "            predictions = []\n",
    "            for f in frames:\n",
    "                output, states = model({**states, 'image': f})\n",
    "                predictions.append(output)\n",
    "                \n",
    "            convert_soft = tf.keras.activations.softmax(predictions[-1])\n",
    "            top_k_vals = tf.math.top_k(convert_soft, k=3, sorted=True, name=None).values[0]\n",
    "            top_k_inds = tf.math.top_k(convert_soft, k=3, sorted=True, name=None).indices[0]\n",
    "            \n",
    "            z = zip(range(0, 3), top_k_inds, top_k_vals)\n",
    "            image = plot_viz(z, KINETICS_600_LABELS, image)\n",
    "            \n",
    "            #out = list(KINETICS_600_LABELS[tf.argmax(predictions[-1], -1)])\n",
    "            \n",
    "#             sentence += out\n",
    "#             if len(sentence) > 1:\n",
    "#                 sentence = sentence[-1:]\n",
    "                \n",
    "            \n",
    "\n",
    "#         if out[np.argmax(out)] > threshold:\n",
    "#             if len(sentence) > 0:\n",
    "#                 if actions[np.argmax(out)] != sentence[-1]:\n",
    "#                     sentence.append(actions[np.argmax(out)])\n",
    "#             else:\n",
    "#                 sentence.append(actions[np.argmax(out)])\n",
    "\n",
    "#         if len(sentence) > 5:\n",
    "#             sentence = sentence[-5:]\n",
    "        \n",
    "        \n",
    "        #cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        #cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # show to screen\n",
    "        cv2.imshow('Translator', image)\n",
    "\n",
    "        # press q to break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cb887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
